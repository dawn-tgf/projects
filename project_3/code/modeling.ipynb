{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Join the two subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = pickle.load(open('../data/pickled_depression.csv', 'rb'))\n",
    "sw = pickle.load(open('../data/pickled_suicide.csv', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(874, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(988, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>comtext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>depression_help</td>\n",
       "      <td>I learned that 78% of suicides are men and peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>depression_help</td>\n",
       "      <td>Those dark days As I write this, reddit sugges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>depression_help</td>\n",
       "      <td>There's nothing interesting to title this I fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>depression_help</td>\n",
       "      <td>School instantly destroys my mental state I st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>depression_help</td>\n",
       "      <td>Yeeeah first shower in...a long time! Yeeeah f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1857</td>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>I wanna die but i can’t. My gf told her mom sb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1858</td>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>22 female can’t get out of bed Sorry there is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1859</td>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>Posted this in r/Depression. Upon reflection i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>i fucking hate psychosis i hate my meds they’v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1861</td>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>I’m in way over my head. Today feels like the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1862 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subreddit                                            comtext\n",
       "0     depression_help  I learned that 78% of suicides are men and peo...\n",
       "1     depression_help  Those dark days As I write this, reddit sugges...\n",
       "2     depression_help  There's nothing interesting to title this I fe...\n",
       "3     depression_help  School instantly destroys my mental state I st...\n",
       "4     depression_help  Yeeeah first shower in...a long time! Yeeeah f...\n",
       "...               ...                                                ...\n",
       "1857     SuicideWatch  I wanna die but i can’t. My gf told her mom sb...\n",
       "1858     SuicideWatch  22 female can’t get out of bed Sorry there is ...\n",
       "1859     SuicideWatch  Posted this in r/Depression. Upon reflection i...\n",
       "1860     SuicideWatch  i fucking hate psychosis i hate my meds they’v...\n",
       "1861     SuicideWatch  I’m in way over my head. Today feels like the ...\n",
       "\n",
       "[1862 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([dh,sw], ignore_index=True, join='outer')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a column for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['suicide'] = [1 if df.loc[i,'subreddit'] == 'SuicideWatch' else 0 for i in range(df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>comtext</th>\n",
       "      <th>suicide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>depression_help</td>\n",
       "      <td>I learned that 78% of suicides are men and peo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>depression_help</td>\n",
       "      <td>Those dark days As I write this, reddit sugges...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>depression_help</td>\n",
       "      <td>There's nothing interesting to title this I fe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>depression_help</td>\n",
       "      <td>School instantly destroys my mental state I st...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>depression_help</td>\n",
       "      <td>Yeeeah first shower in...a long time! Yeeeah f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1857</td>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>I wanna die but i can’t. My gf told her mom sb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1858</td>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>22 female can’t get out of bed Sorry there is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1859</td>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>Posted this in r/Depression. Upon reflection i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>i fucking hate psychosis i hate my meds they’v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1861</td>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>I’m in way over my head. Today feels like the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1862 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subreddit                                            comtext  \\\n",
       "0     depression_help  I learned that 78% of suicides are men and peo...   \n",
       "1     depression_help  Those dark days As I write this, reddit sugges...   \n",
       "2     depression_help  There's nothing interesting to title this I fe...   \n",
       "3     depression_help  School instantly destroys my mental state I st...   \n",
       "4     depression_help  Yeeeah first shower in...a long time! Yeeeah f...   \n",
       "...               ...                                                ...   \n",
       "1857     SuicideWatch  I wanna die but i can’t. My gf told her mom sb...   \n",
       "1858     SuicideWatch  22 female can’t get out of bed Sorry there is ...   \n",
       "1859     SuicideWatch  Posted this in r/Depression. Upon reflection i...   \n",
       "1860     SuicideWatch  i fucking hate psychosis i hate my meds they’v...   \n",
       "1861     SuicideWatch  I’m in way over my head. Today feels like the ...   \n",
       "\n",
       "      suicide  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "1857        1  \n",
       "1858        1  \n",
       "1859        1  \n",
       "1860        1  \n",
       "1861        1  \n",
       "\n",
       "[1862 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Accuracy - 53%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.530612\n",
       "0    0.469388\n",
       "Name: suicide, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.suicide.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to prepare the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_to_words(raw_info):\n",
    "    # Function to convert raw info to a string of words\n",
    "    # The input is astring (raw data), and \n",
    "    # the output is also a string (preprocessed data)\n",
    "    \n",
    "    # 1. Remove non-letters.\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_info)\n",
    "    \n",
    "    # 2. Convert to lower case, split into individual words.\n",
    "    words = letters_only.lower().split()\n",
    "    # Notice that we did this in one line!\n",
    "    \n",
    "    # 3. Convert the stop words to a set to enhance the search spped\n",
    "    # stops = set(stopwords.words('english'))     # stopwords from nltk\n",
    "    stops = set(stop_words.ENGLISH_STOP_WORDS)    # stopwords from sklearn\n",
    "#    stops.update({'depression', 'depressed', 'suicide'})\n",
    "    \n",
    "    # 4. Remove stop words.\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # 5. Lemmatize the words\n",
    "    # Instantiate lemmatizer. (Same as above.)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words_lem = [lemmatizer.lemmatize(i) for i in meaningful_words]\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(words_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing df ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning and parsing df ...\")\n",
    "clean_text = [info_to_words(i) for i in df['comtext']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the text before and after clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nothing Matters I\\'ve been trying to ignore a voice in my head, but I just can\\'t take it anymore. Every night something  says \"kill yourself, kill yourself, kill yourself.\" THE VOICE NEVER GOES AWAY. Nobody cares about me. My only friend ignores me and my parents are always working. I feel like if I hung myself or slit my own throat nobody would even notice. I can\\'t find any reason to live. It\\'ll never get better. People say it will, but it never does. If I stop talking, I\\'m sorry.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.comtext[1800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'matter ve trying ignore voice head just t anymore night say kill kill kill voice go away care friend ignores parent working feel like hung slit throat notice t reason live ll better people say doe stop talking m sorry'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text[1800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    988\n",
       "0    874\n",
       "Name: suicide, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.suicide.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1862,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['suicide'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1862"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "1857    1\n",
       "1858    1\n",
       "1859    1\n",
       "1860    1\n",
       "1861    1\n",
       "Name: suicide, Length: 1862, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.suicide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Test Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_text\n",
    "y = df.suicide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,\n",
    "                                                y,\n",
    "                                                shuffle = True,\n",
    "                                                random_state = 42,\n",
    "                                                test_size = 0.3,\n",
    "                                                stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following models were assessed:\n",
    "    1. LogisticRegression with \n",
    "        - CountVectorizer\n",
    "        - TfidfVectorizer\n",
    "    2. Naive Bays Model with\n",
    "        - CountVectorizer\n",
    "        - TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pipeline the vectorizer and the model\n",
    "- Perform randomizedSearch for the optimal params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TfidfVectorizer is similar to CoutVectorizer, except It tells us which words are most discriminating between documents.  Common words are penalized, but rare words will have more influence.  I'm curious to know if these two vectorizers will have significant impact on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dai_f\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9877206446661551, 0.7298747763864043)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec=CountVectorizer(max_features = 5000)\n",
    "cvec.fit(X_train)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(cvec.transform(X_train), y_train)\n",
    "lr.score(cvec.transform(X_train), y_train), lr.score(cvec.transform(X_test), y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LogisticRegression with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:   39.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7651573292402148\n",
      "{'lr__solver': 'liblinear', 'lr__penalty': 'l1', 'lr__class_weight': None, 'lr__C': 0.2, 'cvec__ngram_range': (1, 2), 'cvec__min_df': 2, 'cvec__max_features': 600, 'cvec__max_df': 0.9}\n"
     ]
    }
   ],
   "source": [
    "pipe_log = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [500,600,700,800,1000,2000,3000, 3500,5000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [0.8, 0.9, 0.95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'lr__penalty':['l1','l2'],\n",
    "    'lr__class_weight': [None, 'balanced'],\n",
    "    'lr__solver':['liblinear'],\n",
    "    'lr__C':[0.1, 0.2,0.3]\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(pipe_log, \n",
    "                        pipe_params, \n",
    "                        cv=5, \n",
    "                        n_iter=50, \n",
    "                        verbose=1, \n",
    "                        n_jobs=-1, \n",
    "                        random_state=42)\n",
    "rs.fit(X_train, y_train)\n",
    "print(rs.best_score_)\n",
    "print(rs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.68      0.73       262\n",
      "           1       0.75      0.85      0.79       297\n",
      "\n",
      "    accuracy                           0.77       559\n",
      "   macro avg       0.77      0.76      0.76       559\n",
      "weighted avg       0.77      0.77      0.76       559\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8242517267843438, 0.7656529516994633)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the best parameters to the pipeline\n",
    "logreg = Pipeline([\n",
    "    ('cvec', CountVectorizer(ngram_range = (1, 2),\n",
    "                            min_df = 2,\n",
    "                            max_df = 0.9,\n",
    "                            max_features = 600)),\n",
    "    ('lr', LogisticRegression(solver = 'liblinear',\n",
    "                             penalty = 'l1',\n",
    "                             class_weight = 'balanced',\n",
    "                             C = 0.2))\n",
    "])\n",
    "\n",
    "logreg.fit(X_train,y_train)\n",
    "pred = logreg.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "# look at the score on the test data\n",
    "logreg.score(X_train,y_train), logreg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_depressed</th>\n",
       "      <th>pred_suicide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual_depressed</td>\n",
       "      <td>177</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual_suicide</td>\n",
       "      <td>46</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  pred_depressed  pred_suicide\n",
       "actual_depressed             177            85\n",
       "actual_suicide                46           251"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, pred)\n",
    "cm_df = pd.DataFrame(cm, columns = ['pred_depressed', 'pred_suicide'], index = ['actual_depressed', 'actual_suicide'] )\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test,pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7470238095238095"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp/(fp+tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8451178451178452"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp/(tp+fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LogisticRegression with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   33.0s\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:   50.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7689946277820414\n",
      "{'tvec__sublinear_tf': False, 'tvec__norm': 'l2', 'tvec__ngram_range': (1, 1), 'tvec__min_df': 2, 'tvec__max_features': 500, 'tvec__max_df': 1.0, 'lr__solver': 'liblinear', 'lr__penalty': 'l2', 'lr__class_weight': 'balanced', 'lr__C': 0.5}\n"
     ]
    }
   ],
   "source": [
    "pipe_log = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'tvec__ngram_range': [(1, 1), (1,2), (1,5)],\n",
    "    'tvec__max_df': [0.8, 0.9,1.0],\n",
    "    'tvec__min_df': [2,3],\n",
    "    'tvec__max_features': [500,600,700,800,1000,2000,3000, 3500,5000],\n",
    "    'tvec__norm': ['l1','l2', None],\n",
    "    'tvec__sublinear_tf': [False, True],\n",
    "    'lr__penalty':['l1','l2'],\n",
    "    'lr__class_weight': [None, 'balanced'],\n",
    "    'lr__solver':['liblinear'],\n",
    "    'lr__C':[0.2,0.3,0.4,0.45, 0.5, 0.55, 0.6,0.7, 0.8]\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(pipe_log, \n",
    "                        pipe_params, \n",
    "                        cv=5, \n",
    "                        n_iter=50, \n",
    "                        verbose=1, \n",
    "                        n_jobs=-1, \n",
    "                        random_state=42)\n",
    "\n",
    "rs.fit(X_train, y_train)\n",
    "print(rs.best_score_)\n",
    "print(rs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.79      0.77       262\n",
      "           1       0.80      0.76      0.78       297\n",
      "\n",
      "    accuracy                           0.77       559\n",
      "   macro avg       0.77      0.78      0.77       559\n",
      "weighted avg       0.78      0.77      0.77       559\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8257866462010744, 0.774597495527728)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the best parameters to the pipeline\n",
    "t_logreg = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(max_features=500,\n",
    "                            min_df = 2,\n",
    "                            max_df = 1.0,\n",
    "                            norm = 'l2',\n",
    "                            sublinear_tf = False)),\n",
    "    ('lr', LogisticRegression(solver = 'liblinear',\n",
    "                             penalty = 'l2',\n",
    "                             class_weight = 'balanced',\n",
    "                             C = 0.5))\n",
    "])\n",
    "\n",
    "t_logreg.fit(X_train,y_train)\n",
    "pred = t_logreg.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "# look at the score on the test data\n",
    "t_logreg.score(X_train,y_train), t_logreg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_depressed</th>\n",
       "      <th>pred_suicide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual_depressed</td>\n",
       "      <td>207</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual_suicide</td>\n",
       "      <td>71</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  pred_depressed  pred_suicide\n",
       "actual_depressed             207            55\n",
       "actual_suicide                71           226"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, pred)\n",
    "cm_df = pd.DataFrame(cm, columns = ['pred_depressed', 'pred_suicide'], index = ['actual_depressed', 'actual_suicide'] )\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score between train and test data are big, indicating the model overfits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1303, 559)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train),len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of X are all integer counts after CountVectorizer, so MultinomialNB is the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Naive Bayes with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   45.7s\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:   58.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7820414428242517\n",
      "{'cvec__ngram_range': (1, 3), 'cvec__min_df': 2, 'cvec__max_features': 1000, 'cvec__max_df': 0.9}\n"
     ]
    }
   ],
   "source": [
    "cvec_nb = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [500,600,700,800,1000,2000,3000, 3500,5000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3), (1,4), (1,5)]\n",
    "}\n",
    "\n",
    "cvec_nb_rs = RandomizedSearchCV(cvec_nb, \n",
    "                        pipe_params, \n",
    "                        cv=5, \n",
    "                        n_iter=50, \n",
    "                        verbose=1, \n",
    "                        n_jobs=-1, \n",
    "                        random_state=42)\n",
    "\n",
    "# Fit and evaluate the model\n",
    "cvec_nb_rs.fit(X_train, y_train)\n",
    "print(cvec_nb_rs.best_score_)\n",
    "print(cvec_nb_rs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.78      0.78       262\n",
      "           1       0.81      0.82      0.81       297\n",
      "\n",
      "    accuracy                           0.80       559\n",
      "   macro avg       0.80      0.80      0.80       559\n",
      "weighted avg       0.80      0.80      0.80       559\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8457405986185725, 0.7996422182468694)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the best parameters to the pipeline\n",
    "cvec_nb = Pipeline([\n",
    "    ('cvec', CountVectorizer(ngram_range=(1, 3),\n",
    "                             min_df = 2,\n",
    "                             max_df = 0.95,\n",
    "                             max_features = 1000,\n",
    "                            )),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit and score the model\n",
    "cvec_nb.fit(X_train,y_train)\n",
    "pred = cvec_nb.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "# look at the score on the train and test data\n",
    "cvec_nb.score(X_train,y_train), cvec_nb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_depressed</th>\n",
       "      <th>pred_suicide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual_depressed</td>\n",
       "      <td>204</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual_suicide</td>\n",
       "      <td>54</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  pred_depressed  pred_suicide\n",
       "actual_depressed             204            58\n",
       "actual_suicide                54           243"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, pred)\n",
    "cm_df = pd.DataFrame(cm, columns = ['pred_depressed', 'pred_suicide'], index = ['actual_depressed', 'actual_suicide'] )\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Naive Bayes with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   29.4s\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:   41.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7835763622409824\n",
      "{'tvec__sublinear_tf': True, 'tvec__norm': 'l2', 'tvec__ngram_range': (1, 2), 'tvec__min_df': 3, 'tvec__max_features': 800, 'tvec__max_df': 0.9}\n"
     ]
    }
   ],
   "source": [
    "pipe_nb = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2), (1,3), (1,4)],\n",
    "    'tvec__max_df': [0.8, 0.9,1.0],\n",
    "    'tvec__min_df': [2,3],\n",
    "    'tvec__max_features': [500,600,700,800,1000,2000,3000, 3500,5000],\n",
    "    'tvec__norm': ['l1','l2', None],\n",
    "    'tvec__sublinear_tf': [False, True],\n",
    "}\n",
    "\n",
    "tvec_nb_rs = RandomizedSearchCV(pipe_nb, \n",
    "                        pipe_params, \n",
    "                        cv=5, \n",
    "                        n_iter=50, \n",
    "                        verbose=1, \n",
    "                        n_jobs=-1, \n",
    "                        random_state=42)\n",
    "\n",
    "# Fit and evaluate the model\n",
    "tvec_nb_rs.fit(X_train, y_train)\n",
    "print(tvec_nb_rs.best_score_)\n",
    "print(tvec_nb_rs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.72      0.75       262\n",
      "           1       0.77      0.82      0.80       297\n",
      "\n",
      "    accuracy                           0.77       559\n",
      "   macro avg       0.78      0.77      0.77       559\n",
      "weighted avg       0.78      0.77      0.77       559\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8403683806600154, 0.774597495527728)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the best parameters to the pipeline\n",
    "tvec_nb = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(ngram_range = (1, 2),\n",
    "                            max_features=800,\n",
    "                            min_df = 3,\n",
    "                            max_df = 0.9,\n",
    "                            norm = 'l2',\n",
    "                            sublinear_tf = True)),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit and score the model\n",
    "tvec_nb.fit(X_train,y_train)\n",
    "pred = tvec_nb.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "# look at the score on the train and test data\n",
    "tvec_nb.score(X_train,y_train), tvec_nb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_depressed</th>\n",
       "      <th>pred_suicide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual_depressed</td>\n",
       "      <td>188</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual_suicide</td>\n",
       "      <td>52</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  pred_depressed  pred_suicide\n",
       "actual_depressed             188            74\n",
       "actual_suicide                52           245"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, pred)\n",
    "cm_df = pd.DataFrame(cm, columns = ['pred_depressed', 'pred_suicide'], index = ['actual_depressed', 'actual_suicide'] )\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below summarizes the performance of each combination of vectorizer and models.\n",
    "\n",
    "|No.|Model|Train Accuracy|Test Accuracy|f1_score|recall|Specificity|\n",
    "|---|---|---|---|---|---|---|\n",
    "|1.|CVec + LogReg|0.82|0.77|0.79|0.85|0.68|\n",
    "|2.|TfidfVec + LogReg|0.83|0.77|0.78|0.76|0.79|\n",
    "|3.|CVec + NB|0.84|0.80|0.81|0.81|0.78|\n",
    "|4.|TfidfVec + NB|0.84|0.77|0.80|0.82|0.72|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "1. Both models are overfitting the data, as train accuracy score is always higher than the test score.\n",
    "2. Both models only scored in the 77%-80% test accuracy, which means that our models can't accurately predict a post.\n",
    "3. Naive Bayes model performs slightly better than Logistic Regression in test accuracy though.\n",
    "3. Naive bayes model performs also better than Logistic Regression in f1_score in general.\n",
    "\n",
    "The positive is \"SuicideWatch\". In this case, as our goal is to prevent suicide, **recall** (or **sensitivity**) is used to evaluate the models, which means we'd like to minimize false negative (i.e. avoiding scenario when someone with suicidal thoughts is incorrectly predicted as just having depression, hence a miss case). \n",
    "\n",
    "For this reason, combination #1 (CountVectorizer and Logistic Regression) is considered the best model, as it has the highest recall. Its recall and specificity are also farely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords to predict a SuicideWatch post in the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "logreg_coef_list = pd.DataFrame(zip(np.exp(logreg[1].coef_[0]),logreg[0].get_feature_names()), columns = ['coef', 'word'])\n",
    "logreg_coef_list.sort_values(\"coef\", ascending=False).head(20).plot(kind='barh', x='word')\n",
    "plt.savefig('../images/c_logreg_top_coef.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If someone mentions \"kill\" in his post, he is 2.5 times as likely to have suicide intent, with all else held equal.\n",
    "- Other dominant keywords to predict a SuicideWatch post are: suicide, suicidal, and die - which are not surprising.\n",
    "- Interestingly, a SuicideWatch post will also tend to mention the word \"matter\", \"okay\", \"ex\" and \"dad\".  The posts likely mention phases like \"doesn't matter\", \"none of it matters\", \"it's okay\", etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Recommendataion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the models generalize well. The best model using Countervectorizer and LogisticRegression only has 77% of accuracy and 85% of sensitivity after optimizing both vectorizer and the model.  This can be explained by the similar contents, the high overlaps of the words seen in both subreddits.  In order to have better results, other classifiers such as SVM or Random Forest could be explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V: Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier is able to predict a SuicideWatch post with an accuracy of 77% and sensitivity of 85%, which isn't fantastic. However what is learnt from this project is:\n",
    " - 1) If someone mentions \"suicide\", \"suicidal\" or \"kill\", or \"die\" in a post, we shouldn't ignore the potential of suicidal intention, for very obvious reasons.\n",
    " - 2) A suicidal person tends to say things like \"doesn't matter\", or \"it's okay\" in his post, which is the exact opposite of the situation.  \n",
    " - 3) A suicidal person also seems to mention \"ex\" and \"dad\" - as I'm not a domain expert in psychology, but it's natural to guess that father has some sort of influence, and relationship is one of the reasons of suicides.  This can be verified with the domain experts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
